{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Selamat Datang di Halaman Tugas Penambangan Data Nama : Julius Agung S. NIM : 180411100024 Kelas : Penambangan Data 3D Prodi : Teknik Informatika Angkatan: 2018 Alamat: Perum Sumbertaman Indah Blok RE II No. 19 Kota Probolinggo","title":"Home"},{"location":"#selamat-datang-di-halaman-tugas-penambangan-data","text":"Nama : Julius Agung S. NIM : 180411100024 Kelas : Penambangan Data 3D Prodi : Teknik Informatika Angkatan: 2018 Alamat: Perum Sumbertaman Indah Blok RE II No. 19 Kota Probolinggo","title":"Selamat Datang di Halaman Tugas Penambangan Data"},{"location":"Clustering/","text":"CLUSTERING Clustering adalah metode penganalisaan data yang sering dimasukkan sebagai salah satu metode Data Mining yang tujuannya adalah untuk mengelompokkan data dengan karakteristik yang sama ke suatu wilayah yang sama dan data dengan karakteristik yang berbeda ke wilayah yang lain. \u200b Ada beberapa pendekatan yang digunakan dalam mengembangkan metode clustering, dua pendekatan utama adalah clustering dengan pendekatan partisi dan clustering dengan pendekatan hirarki. Clustering dengan pendekatan partisi atau sering disebut dengan partition-based clustering mengelompokkan data dengan memilah-milah data yang dianalisa ke dalam cluster-cluster yang ada. Clustering dengan pendekatan hirarki atau sering disebut dengan hierarchical clustering mengelompokkan data dengan membuat suatu hirarki berupa dendogram dimana data yang mirip akan ditempatkan pada hirarki yang berdekatan dan yang tidak pada hirarki yang berjauhan. Di samping kedua pendekatan tersebut, ada juga clustering dengan pendekatan automatic mapping (Self-Organising Map/SOM). Clustering dengan pendekatan partisi 1. K-Means \u200b Salah satu metode yang banyak digunakan dalam melakukan clustering dengan partisi ini adalah metode k-means. Secara umum metode k-means ini melakukan proses pengelompokan dengan prosedur sebagai berikut: \u00b7 Tentukan jumlah cluster \u00b7 Alokasikan data secara random ke cluster yang ada \u00b7 Hitung rata-rata setiap cluster dari data yang tergabung di dalamnya \u00b7 Alokasikan kembali semua data ke cluster terdekat \u00b7 Ulang proses nomor 3, sampai tidak ada perubahan atau perubahan yang terjadi masih sudah di bawah treshold \u200b Prosedur dasar ini bisa berubah mengikuti pendekatan pengalokasian data yang diterapkan, apakah crisp atau fuzzy . Setelah meneliti clustering dari sudut yang lain, saya menemukan bahwa k-means clustering mempunyai beberapa kelemahan. Fungsi dari algoritma ini adalah mengelompokkan data kedalam beberapa cluster. karakteristik dari algoritma ini adalah : . Memiliki n buah data. . Input berupa jumlah data dan jumlah cluster (kelompok). . Pada setiap cluster/kelompok memiliki sebuah centroid yang mempresentasikan cluster tersebut. Algoritma K-Means \u200b Secara sederhana algoritma K-Means dimulai dari tahap berikut : . Pilih K buah titik centroid. . Menghitung jarak data dengan centroid. . Update nilai titik centroid. . Ulangi langkah 2 dan 3 sampai nilai dari titik centroid tidak lagi berubah. Rumus K-Means Metode K-Modes \u200b K-Modes merupakan pengembangan dari algoritma clustering K-means untuk menangani data kategorik di mana means diganti oleh modes. K-Modes menggunakan simple matching meassure dalam penentuan similarity dari suatu klaster. Metode K-Prototype \u200b Tujuan dari simulasi ini adalah mencoba menerapkan algoritma K-Prototype pada data campuran numerik dan kategorikal. Ada tahap preparation diperlakukan terhadap data point numerik normalisasi terlebih dahulu. Algoritma K-Prototype \u200b Sebelum masuk proses algoritma K-Prototypes tentukan jumlah k yang akan dibentuk batasannya minimal 2 dan maksimal \u221an atau n/2 dimana n adalah jumlah data point atau obyek . Tahap 1 : Tentukan K dengan inisial kluster z1, z2, ..., zk secara acak dari n buah titik {x1, x2, ..., xn} . Tahap 2 : Hitung jarak seluruh data point pada data set terhadap inisial kluster awal, alokasikan data point ke dalam cluster yang memiliki jarak prototype terdekat dengan object yang diukur. . Tahap 3 : Hitung titik pusat cluster yang baru setelah semua objek dialokasikan. Lalu realokasikan semua datapoint pada dataset terhadap prototype yang baru. . Tahap 4 : jika titik pusat cluster tidak berubah atau sudah konvergen maka proses algoritma berhenti tetapi jika titik pusat masih berubah-ubah secara signifikan maka proses kembali ke tahap 2 dan 3 hingga iterasi maksimum tercapai atau sudah tidak ada perpindahan objek. Rumus K-Prototype 2. Mixture Modelling (Mixture Modeling) \u200b Mixture modelling merupakan metode pengelompokan data yang mirip dengan k-means dengan kelebihan penggunaan distribusi statistik dalam mendefinisikan setiap cluster yang ditemukan. Dibandingkan dengan k-means yang hanya menggunakan cluster center, penggunaan distribusi statistik ini mengijinkan kita untuk: \u00b7 Memodel data yang kita miliki dengan setting karakteristik yang berbeda-beda \u00b7 Jumlah cluster yang sesuai dengan keadaan data bisa ditemukan seiring dengan proses pemodelan karakteristik dari masing-masing cluster \u00b7 Hasil pemodelan clustering yang dilaksanakan bisa diuji tingkat keakuratannya \u200b Distribusi statistik yang digunakan bisa bermacam-macam mulai dari yang digunakan untuk data categorical sampai yang continuous, termasuk di antaranya distribusi binomial, multinomial, normal dan lain-lain. Beberapa distribusi yang bersifat tidak normal seperti distribusi Poisson, von-Mises, Gamma dan Student t, juga diterapkan untuk bisa mengakomodasi berbagai keadaan data yang ada di lapangan. Beberapa pendekatan multivariate juga banyak diterapkan untuk memperhitungkan tingkat keterkaitan antara variabel data yang satu dengan yang lainnya. Clustering dengan Pendekatan Hirarki \u200b Clustering dengan pendekatan hirarki mengelompokkan data yang mirip dalam hirarki yang sama dan yang tidak mirip di hirarki yang agak jauh. Ada dua metode yang sering diterapkan yaitu agglomerative hieararchical clustering dan divisive hierarchical clustering . Agglomerative melakukan proses clustering dari N cluster menjadi satu kesatuan cluster, dimana N adalah jumlah data, sedangkan divisive melakukan proses clustering yang sebaliknya yaitu dari satu cluster menjadi N cluster. \u200b Beberapa metode hierarchical clustering yang sering digunakan dibedakan menurut cara mereka untuk menghitung tingkat kemiripan. Ada yang menggunakan Single Linkage , Complete Linkage , Average Linkage , Average Group Linkage dan lain-lainnya. Seperti juga halnya dengan partition-based clustering , kita juga bisa memilih jenis jarak yang digunakan untuk menghitung tingkat kemiripan antar data. \u200b Salah satu cara untuk mempermudah pengembangan dendogram untuk hierarchical clustering ini adalah dengan membuat similarity matrix yang memuat tingkat kemiripan antar data yang dikelompokkan. Tingkat kemiripan bisa dihitung dengan berbagai macam cara seperti dengan Euclidean Distance Space. Berangkat dari similarity matrix ini, kita bisa memilih lingkage jenis mana yang akan digunakan untuk mengelompokkan data yang dianalisa. Clustering Dengan Pendekatan Automatic Mapping (Self-Organising Map/SOM) \u200b Self-Organising Map merupakan suatu tipe Artificial Neural Networks yang di-training secara unsupervised. SOM menghasilkan map yang terdiri dari output dalam dimensi yang rendah (2 atau 3 dimensi). Map ini berusaha mencari property dari input data. Komposisi input dan output dalam SOM mirip dengan komposisi dari proses feature scaling (multidimensional scaling). \u200b Walaupun proses learning yang dilakukan mirip dengan Artificial Neural Networks, tetapi proses untuk meng-assign input data ke map, lebih mirip dengan K-Means dan kNN Algorithm. Adapun prosedur yang ditempuh dalam melakukan clustering dengan SOM adalah sebagai berikut: \u00b7 Tentukan weight dari input data secara random \u00b7 Pilih salah satu input data \u00b7 Hitung tingkat kesamaan (dengan Eucledian) antara input data dan weight dari input data tersebut dan pilih input data yang memiliki kesamaan dengan weight yang ada (data ini disebut dengan Best Matching Unit (BMU)) \u00b7 Perbaharui weight dari input data dengan mendekatkan weight tersebut ke BMU dengan rumus: Wv(t+1) = Wv(t) + Theta(v, t) x Alpha(t) x (D(t) \u2013 Wv(t)) Dimana: o Wv(t) : Weight pada saat ke-t o Theta (v, t) : Fungsi neighbourhood yang tergantung pada Lattice distance antara BMU dengan neuron v. Umumnya bernilai 1 untuk neuron yang cukup dekat dengan BMU, dan 0 untuk yang sebaliknya. Penggunaan fungsi Gaussian juga memungkinkan. o Alpha (t) : Learning Coefficient yang berkurang secara monotonic o D(t) : Input data \u00b7 Tambah nilai t, sampai t < Lambda , dimana Lambda adalah jumlah iterasi MathJax.Hub.Config({ tex2jax: {inlineMath: [['$$','$$']]} });","title":"Clustering"},{"location":"Clustering/#clustering","text":"Clustering adalah metode penganalisaan data yang sering dimasukkan sebagai salah satu metode Data Mining yang tujuannya adalah untuk mengelompokkan data dengan karakteristik yang sama ke suatu wilayah yang sama dan data dengan karakteristik yang berbeda ke wilayah yang lain. \u200b Ada beberapa pendekatan yang digunakan dalam mengembangkan metode clustering, dua pendekatan utama adalah clustering dengan pendekatan partisi dan clustering dengan pendekatan hirarki. Clustering dengan pendekatan partisi atau sering disebut dengan partition-based clustering mengelompokkan data dengan memilah-milah data yang dianalisa ke dalam cluster-cluster yang ada. Clustering dengan pendekatan hirarki atau sering disebut dengan hierarchical clustering mengelompokkan data dengan membuat suatu hirarki berupa dendogram dimana data yang mirip akan ditempatkan pada hirarki yang berdekatan dan yang tidak pada hirarki yang berjauhan. Di samping kedua pendekatan tersebut, ada juga clustering dengan pendekatan automatic mapping (Self-Organising Map/SOM).","title":"CLUSTERING"},{"location":"Clustering/#clustering-dengan-pendekatan-partisi","text":"1. K-Means \u200b Salah satu metode yang banyak digunakan dalam melakukan clustering dengan partisi ini adalah metode k-means. Secara umum metode k-means ini melakukan proses pengelompokan dengan prosedur sebagai berikut: \u00b7 Tentukan jumlah cluster \u00b7 Alokasikan data secara random ke cluster yang ada \u00b7 Hitung rata-rata setiap cluster dari data yang tergabung di dalamnya \u00b7 Alokasikan kembali semua data ke cluster terdekat \u00b7 Ulang proses nomor 3, sampai tidak ada perubahan atau perubahan yang terjadi masih sudah di bawah treshold \u200b Prosedur dasar ini bisa berubah mengikuti pendekatan pengalokasian data yang diterapkan, apakah crisp atau fuzzy . Setelah meneliti clustering dari sudut yang lain, saya menemukan bahwa k-means clustering mempunyai beberapa kelemahan. Fungsi dari algoritma ini adalah mengelompokkan data kedalam beberapa cluster. karakteristik dari algoritma ini adalah : . Memiliki n buah data. . Input berupa jumlah data dan jumlah cluster (kelompok). . Pada setiap cluster/kelompok memiliki sebuah centroid yang mempresentasikan cluster tersebut.","title":"Clustering dengan pendekatan partisi"},{"location":"Clustering/#algoritma-k-means","text":"\u200b Secara sederhana algoritma K-Means dimulai dari tahap berikut : . Pilih K buah titik centroid. . Menghitung jarak data dengan centroid. . Update nilai titik centroid. . Ulangi langkah 2 dan 3 sampai nilai dari titik centroid tidak lagi berubah.","title":"Algoritma K-Means"},{"location":"Clustering/#rumus-k-means","text":"","title":"Rumus K-Means"},{"location":"Clustering/#metode-k-modes","text":"\u200b K-Modes merupakan pengembangan dari algoritma clustering K-means untuk menangani data kategorik di mana means diganti oleh modes. K-Modes menggunakan simple matching meassure dalam penentuan similarity dari suatu klaster.","title":"Metode K-Modes"},{"location":"Clustering/#metode-k-prototype","text":"\u200b Tujuan dari simulasi ini adalah mencoba menerapkan algoritma K-Prototype pada data campuran numerik dan kategorikal. Ada tahap preparation diperlakukan terhadap data point numerik normalisasi terlebih dahulu.","title":"Metode K-Prototype"},{"location":"Clustering/#algoritma-k-prototype","text":"\u200b Sebelum masuk proses algoritma K-Prototypes tentukan jumlah k yang akan dibentuk batasannya minimal 2 dan maksimal \u221an atau n/2 dimana n adalah jumlah data point atau obyek . Tahap 1 : Tentukan K dengan inisial kluster z1, z2, ..., zk secara acak dari n buah titik {x1, x2, ..., xn} . Tahap 2 : Hitung jarak seluruh data point pada data set terhadap inisial kluster awal, alokasikan data point ke dalam cluster yang memiliki jarak prototype terdekat dengan object yang diukur. . Tahap 3 : Hitung titik pusat cluster yang baru setelah semua objek dialokasikan. Lalu realokasikan semua datapoint pada dataset terhadap prototype yang baru. . Tahap 4 : jika titik pusat cluster tidak berubah atau sudah konvergen maka proses algoritma berhenti tetapi jika titik pusat masih berubah-ubah secara signifikan maka proses kembali ke tahap 2 dan 3 hingga iterasi maksimum tercapai atau sudah tidak ada perpindahan objek.","title":"Algoritma K-Prototype"},{"location":"Clustering/#rumus-k-prototype","text":"2. Mixture Modelling (Mixture Modeling) \u200b Mixture modelling merupakan metode pengelompokan data yang mirip dengan k-means dengan kelebihan penggunaan distribusi statistik dalam mendefinisikan setiap cluster yang ditemukan. Dibandingkan dengan k-means yang hanya menggunakan cluster center, penggunaan distribusi statistik ini mengijinkan kita untuk: \u00b7 Memodel data yang kita miliki dengan setting karakteristik yang berbeda-beda \u00b7 Jumlah cluster yang sesuai dengan keadaan data bisa ditemukan seiring dengan proses pemodelan karakteristik dari masing-masing cluster \u00b7 Hasil pemodelan clustering yang dilaksanakan bisa diuji tingkat keakuratannya \u200b Distribusi statistik yang digunakan bisa bermacam-macam mulai dari yang digunakan untuk data categorical sampai yang continuous, termasuk di antaranya distribusi binomial, multinomial, normal dan lain-lain. Beberapa distribusi yang bersifat tidak normal seperti distribusi Poisson, von-Mises, Gamma dan Student t, juga diterapkan untuk bisa mengakomodasi berbagai keadaan data yang ada di lapangan. Beberapa pendekatan multivariate juga banyak diterapkan untuk memperhitungkan tingkat keterkaitan antara variabel data yang satu dengan yang lainnya.","title":"Rumus K-Prototype"},{"location":"Clustering/#clustering-dengan-pendekatan-hirarki","text":"\u200b Clustering dengan pendekatan hirarki mengelompokkan data yang mirip dalam hirarki yang sama dan yang tidak mirip di hirarki yang agak jauh. Ada dua metode yang sering diterapkan yaitu agglomerative hieararchical clustering dan divisive hierarchical clustering . Agglomerative melakukan proses clustering dari N cluster menjadi satu kesatuan cluster, dimana N adalah jumlah data, sedangkan divisive melakukan proses clustering yang sebaliknya yaitu dari satu cluster menjadi N cluster. \u200b Beberapa metode hierarchical clustering yang sering digunakan dibedakan menurut cara mereka untuk menghitung tingkat kemiripan. Ada yang menggunakan Single Linkage , Complete Linkage , Average Linkage , Average Group Linkage dan lain-lainnya. Seperti juga halnya dengan partition-based clustering , kita juga bisa memilih jenis jarak yang digunakan untuk menghitung tingkat kemiripan antar data. \u200b Salah satu cara untuk mempermudah pengembangan dendogram untuk hierarchical clustering ini adalah dengan membuat similarity matrix yang memuat tingkat kemiripan antar data yang dikelompokkan. Tingkat kemiripan bisa dihitung dengan berbagai macam cara seperti dengan Euclidean Distance Space. Berangkat dari similarity matrix ini, kita bisa memilih lingkage jenis mana yang akan digunakan untuk mengelompokkan data yang dianalisa.","title":"Clustering dengan Pendekatan Hirarki"},{"location":"Clustering/#clustering-dengan-pendekatan-automatic-mapping-self-organising-mapsom","text":"\u200b Self-Organising Map merupakan suatu tipe Artificial Neural Networks yang di-training secara unsupervised. SOM menghasilkan map yang terdiri dari output dalam dimensi yang rendah (2 atau 3 dimensi). Map ini berusaha mencari property dari input data. Komposisi input dan output dalam SOM mirip dengan komposisi dari proses feature scaling (multidimensional scaling). \u200b Walaupun proses learning yang dilakukan mirip dengan Artificial Neural Networks, tetapi proses untuk meng-assign input data ke map, lebih mirip dengan K-Means dan kNN Algorithm. Adapun prosedur yang ditempuh dalam melakukan clustering dengan SOM adalah sebagai berikut: \u00b7 Tentukan weight dari input data secara random \u00b7 Pilih salah satu input data \u00b7 Hitung tingkat kesamaan (dengan Eucledian) antara input data dan weight dari input data tersebut dan pilih input data yang memiliki kesamaan dengan weight yang ada (data ini disebut dengan Best Matching Unit (BMU)) \u00b7 Perbaharui weight dari input data dengan mendekatkan weight tersebut ke BMU dengan rumus: Wv(t+1) = Wv(t) + Theta(v, t) x Alpha(t) x (D(t) \u2013 Wv(t)) Dimana: o Wv(t) : Weight pada saat ke-t o Theta (v, t) : Fungsi neighbourhood yang tergantung pada Lattice distance antara BMU dengan neuron v. Umumnya bernilai 1 untuk neuron yang cukup dekat dengan BMU, dan 0 untuk yang sebaliknya. Penggunaan fungsi Gaussian juga memungkinkan. o Alpha (t) : Learning Coefficient yang berkurang secara monotonic o D(t) : Input data \u00b7 Tambah nilai t, sampai t < Lambda , dimana Lambda adalah jumlah iterasi MathJax.Hub.Config({ tex2jax: {inlineMath: [['$$','$$']]} });","title":"Clustering Dengan Pendekatan Automatic Mapping (Self-Organising Map/SOM)"},{"location":"Fuzzy_C-Means/","text":"Fuzzy C-Means \u200b Fuzzy C-Means (FCM) adalah suatu teknik pengelompokan data yang keberadaan tiap-tiap data dalam suatu kelompok ditentukan oleh nilai atau derajat keanggotaan tertentu dan teknik ini pertama kali diperkenalkan oleh Jim Bezdek pada tahun 1981. Fuzzy C-Means menerapkan pengelompokan fuzzy, dimana setiap data dapat menjadi anggota dari beberapa cluster dengan derajat keanggotaan yang berbeda-beda pada setiap cluster. \u200b Fuzzy C-Means merupakan algoritma iteratif, yang menerapkan iterasi pada proses clustering data. Tujuan dari Fuzzy C-Means adalah untuk mendapatkan pusat cluster yang nantinya akan digunakan untuk mengetahui data yang masuk ke dalam sebuah cluster. Dalam teori fuzzy, keanggotaan sebuah data tidak diberikan nilai secara tegas dengan nilai 1(menjadi anggota) dan nilai 0 (tidak menjadi anggota), melaikan dengan suatu nilai derajat keanggotaannya yang jangkauan nilainya 0 sampai 1. Nilai keanggotaan suatu data dalam sebuah himpunan menjadi 0 ketika sama sekali tidak menjadi anggota dan menjadi 1 ketika menjadi anggota secara penuh dalam suatu himpunan.Umumnya nilai keanggotaannya antara 0 sampai 1. semakin tinggi nilai keanggotaanya semakin tinggi derajat keanggotaanya dan semakin kecil maka semakin rendah derajat keanggotaanya. Kaitannya dengan K-means sebenarnya FCM merupakan versi fuzzy dan k-meansdengan beberapa modifikasi yang membedakan dengen K-Means. \u200b Konsep dari Fuzzy C-Means pertama kali adalah menentukan pusat cluster, yang akan menandai lokasi rata-rata untuk tiap-tiap cluster. Pada kondisi awal, pusat cluster ini masih belum akurat. Tiap-tiap titik data memiliki derajat keanggotaan untuk tiap-tiap cluster. Dengan cara memperbaiki pusat cluster dan derajat keanggotaan tiap-tiap titik data secara berulang, maka akan dapat dilihat bahwa pusat cluster akan bergerak menuju lokasi yang tepat. Perulangan ini didasarkan pada minimasi fungsi obyektif yang menggambarkan jarak dari titik data yang diberikan kepusat cluster yang terbobot oleh derajat keanggotaan titik data tersebut. Output dari Fuzzy C-Means merupakan deretan usat cluster dan beberapa derajat keanggotaan untuk tiap-tiap titik data. Informasi ini dapat digunakan untuk membangun suatu fuzzy inference system. Kelebihan dari metode fuzzy C-means adalah sederhana, mudah diimplementasikan, memiliki kemampuan untuk mengelompokkan data yang besar, dan Running timenya linear O( linear O(NCT). Algoritma fuzzy c-means from fcmeans import FCM from sklearn.datasets import make_blobs from matplotlib import pyplot as plt from seaborn import scatterplot as scatter # create artifitial dataset n_samples = 50000 n_bins = 3 # use 3 bins for calibration_curve as we have 3 clusters here centers = [(-5, -5), (0, 0), (5, 5)] X,_ = make_blobs(n_samples=n_samples, n_features=2, cluster_std=1.0, centers=centers, shuffle=False, random_state=42) # fit the fuzzy-c-means fcm = FCM(n_clusters=3) fcm.fit(X) # outputs fcm_centers = fcm.centers fcm_labels = fcm.u.argmax(axis=1) # plot result %matplotlib inline f, axes = plt.subplots(1, 2, figsize=(11,5)) scatter(X[:,0], X[:,1], ax=axes[0]) scatter(X[:,0], X[:,1], ax=axes[1], hue=fcm_labels) scatter(fcm_centers[:,0], fcm_centers[:,1], ax=axes[1],marker=\"s\",s=200) plt.show() MathJax.Hub.Config({ tex2jax: {inlineMath: [['$$','$$']]} });","title":"Fuzzy C-Mean"},{"location":"Fuzzy_C-Means/#fuzzy-c-means","text":"\u200b Fuzzy C-Means (FCM) adalah suatu teknik pengelompokan data yang keberadaan tiap-tiap data dalam suatu kelompok ditentukan oleh nilai atau derajat keanggotaan tertentu dan teknik ini pertama kali diperkenalkan oleh Jim Bezdek pada tahun 1981. Fuzzy C-Means menerapkan pengelompokan fuzzy, dimana setiap data dapat menjadi anggota dari beberapa cluster dengan derajat keanggotaan yang berbeda-beda pada setiap cluster. \u200b Fuzzy C-Means merupakan algoritma iteratif, yang menerapkan iterasi pada proses clustering data. Tujuan dari Fuzzy C-Means adalah untuk mendapatkan pusat cluster yang nantinya akan digunakan untuk mengetahui data yang masuk ke dalam sebuah cluster. Dalam teori fuzzy, keanggotaan sebuah data tidak diberikan nilai secara tegas dengan nilai 1(menjadi anggota) dan nilai 0 (tidak menjadi anggota), melaikan dengan suatu nilai derajat keanggotaannya yang jangkauan nilainya 0 sampai 1. Nilai keanggotaan suatu data dalam sebuah himpunan menjadi 0 ketika sama sekali tidak menjadi anggota dan menjadi 1 ketika menjadi anggota secara penuh dalam suatu himpunan.Umumnya nilai keanggotaannya antara 0 sampai 1. semakin tinggi nilai keanggotaanya semakin tinggi derajat keanggotaanya dan semakin kecil maka semakin rendah derajat keanggotaanya. Kaitannya dengan K-means sebenarnya FCM merupakan versi fuzzy dan k-meansdengan beberapa modifikasi yang membedakan dengen K-Means. \u200b Konsep dari Fuzzy C-Means pertama kali adalah menentukan pusat cluster, yang akan menandai lokasi rata-rata untuk tiap-tiap cluster. Pada kondisi awal, pusat cluster ini masih belum akurat. Tiap-tiap titik data memiliki derajat keanggotaan untuk tiap-tiap cluster. Dengan cara memperbaiki pusat cluster dan derajat keanggotaan tiap-tiap titik data secara berulang, maka akan dapat dilihat bahwa pusat cluster akan bergerak menuju lokasi yang tepat. Perulangan ini didasarkan pada minimasi fungsi obyektif yang menggambarkan jarak dari titik data yang diberikan kepusat cluster yang terbobot oleh derajat keanggotaan titik data tersebut. Output dari Fuzzy C-Means merupakan deretan usat cluster dan beberapa derajat keanggotaan untuk tiap-tiap titik data. Informasi ini dapat digunakan untuk membangun suatu fuzzy inference system. Kelebihan dari metode fuzzy C-means adalah sederhana, mudah diimplementasikan, memiliki kemampuan untuk mengelompokkan data yang besar, dan Running timenya linear O( linear O(NCT).","title":"Fuzzy C-Means"},{"location":"Fuzzy_C-Means/#algoritma-fuzzy-c-means","text":"from fcmeans import FCM from sklearn.datasets import make_blobs from matplotlib import pyplot as plt from seaborn import scatterplot as scatter # create artifitial dataset n_samples = 50000 n_bins = 3 # use 3 bins for calibration_curve as we have 3 clusters here centers = [(-5, -5), (0, 0), (5, 5)] X,_ = make_blobs(n_samples=n_samples, n_features=2, cluster_std=1.0, centers=centers, shuffle=False, random_state=42) # fit the fuzzy-c-means fcm = FCM(n_clusters=3) fcm.fit(X) # outputs fcm_centers = fcm.centers fcm_labels = fcm.u.argmax(axis=1) # plot result %matplotlib inline f, axes = plt.subplots(1, 2, figsize=(11,5)) scatter(X[:,0], X[:,1], ax=axes[0]) scatter(X[:,0], X[:,1], ax=axes[1], hue=fcm_labels) scatter(fcm_centers[:,0], fcm_centers[:,1], ax=axes[1],marker=\"s\",s=200) plt.show() MathJax.Hub.Config({ tex2jax: {inlineMath: [['$$','$$']]} });","title":"Algoritma fuzzy c-means"},{"location":"Mengukur Jarak Data/","text":"Mengukur Jarak Data Mengukur Jarak Tipe Numerik \u00b6 Salah satu tantangan dalam era ini dengan datatabase yang memiliki banyak tipe data. Mengukur jarak adalah komponen utama dalam algoritma clustering berbasis jarak. Alogritma seperti Algoritma Partisioning misal K-Mean, K-medoidm dan fuzzy c-mean dan rough clustering bergantung pada jarak untuk melakukan pengelompokkan. Ada beberapa cara untuk menghitung similaritas atau jarak dari dari data tipe numerik, diantaranya: Minkowski Distance \u00b6 Kelompok Minkowski diantaranya adalah Euclidean distance dan Manhattan distance, yang menjadi kasus khusus dari Minkowski distance. Minkowski distance dinyatakan dengan: dmin=( sumni=1|xi\u2212yi|m)1m,m\u22651dmin=( sumi=1n|xi\u2212yi|m)1m,m\u22651 Diman mm adalah bilangan riel positif dan xixi dan yiyi adalah dua vektor dalam runang dimensi nn Implementasi ukuran jarak Minkowski pada model clustering data atribut dilakukan normalisasi untuk menghindari dominasi dari atribut yang memiliki skala data besar. Manhattan Distance \u00b6 Manhattan distance adalah kasus khsusu dari jarak Minkowski distance pada m = 1. Seperti Minkowski Distance, Manhattan distance sensitif terhadap outlier. BIla ukuran ini digunakan dalam algoritma clustering , bentuk cluster adalah hyper-rectangular. Ukuran ini didefinisikan dengan: dman=n\u2211i=1|xi\u2212yi|dman=\u2211i=1n|xi\u2212yi| Euiclidian Distance \u00b6 Jarak yang paling terkenal yang digunakan untuk data numerik adalah jarak Euclidean. Ini adalah kasus khusus dari jarak Minkowski ketika m = 2. Jarak Euclidean berkinerja baik ketika digunakan untuk kumpulan data cluster kompak atau terisolasi . Meskipun jarak Euclidean sangat umum dalam pengelompokan, ia memiliki kelemahan: jika dua vektor data tidak memiliki nilai atribut yang sama, kemungkin memiliki jarak yang lebih kecil daripada pasangan vektor data lainnya yang mengandung nilai atribut yang sama. Masalah lain dengan jarak Euclidean sebagai fitur skala terbesar akan mendominasi yang lain. Normalisasi fitur kontinu adalah solusi untuk mengatasi kelemahan ini. Euclidian Distance dinyatakan dengan: d(x,y)=\ue001\ue000 \ue000\u23b7n\u2211i=1(xi\u2212yi)2d(x,y)=\u2211i=1n(xi\u2212yi)2 Average Distance \u00b6 Berkenaan dengan kekurangan dari Jarak Euclidian Distance diatas, rata rata jarak adala versi modikfikasi dari jarak Euclidian untuk memperbaiki hasil. Untuk dua titik x,yx,y dalam ruang dimensi nn, rata-rata jarak didefinisikan dengan: dave=(1nn\u2211i=1(xi\u2212yi)2)12dave=(1n\u2211i=1n(xi\u2212yi)2)12 Weighted euclidean distance \u00b6 Jika berdasarkan tingkatan penting dari masing masing atribut ditentukan, maka Weighted Euclidean distance adalah modifikisasi lain dari jarak Euclidean distance yang dapat digunakan. Ukuran ini dirumuskan dengan:dwe=(n\u2211i=1wi(xi\u2212yi)2)12dwe=(\u2211i=1nwi(xi\u2212yi)2)12dimana wi adalah bobot yang diberikan pada atribut ke i. Chord distance \u00b6 Chord distance adalah salah satu ukuran jarak modifikasi Euclidean distance untuk mengatasi kekurangan dari Euclidean distance. Ini dapat dipecahkan juga dengan menggunakan skala pengukuran yang baik. Jarak ini dapat juga dihitung dari data yang tidak dinormalisasi . Chord distance didefinisikan dengan: dchord=(2\u22122\u2211ni=1xiyi|x|2|y|2)12dchord=(2\u22122\u2211i=1nxiyi|x|2|y|2)12dimana |x|2|x|2 adalah L2\u2212norm|x|2=\u221a\u2211ni=1x2iL2\u2212norm|x|2=\u2211i=1nxi2 Mahalanobis distance \u00b6 Mahalanobis distance berdasarkan data berbeda dengan Euclidean dan Manhattan distances yang bebas antra data dengan data yang lain. Jarak Mahalanobis yang teratur dapat digunakan untuk mengekstraksi hyperellipsoidal clusters. Jarak Mahalanobis dapat mengurangi distorsi yang disebabkan oleh korelasi linier antara fitur dengan menerapkan transformasi pemutihan ke data atau dengan menggunakan kuadrat Jarak mahalanobis. Mahalanobis distance dinyatakan dengan: dmah=\u221a(x\u2212y)S\u22121(x\u2212y)Tdmah=(x\u2212y)S\u22121(x\u2212y)Tdiman SS adalah matrik covariance data. Cosine measure \u00b6 Ukuran Cosine similarity lebih banyak digunakan dalam similaritas dokumen dan dinyatakan denganCosine(x,y)=\u2211ni=1xiyi|x|2|y|2Cosine(x,y)=\u2211i=1nxiyi|x|2|y|2dimana |y|2|y|2 adalah Euclidean norm dari vektor y=(y1,y2,\u2026,yn)y=(y1,y2,\u2026,yn) di definisikan dengan |y|2=\u221ay21+y22+\u2026+y2n|y|2=y12+y22+\u2026+yn2 Pearson Correlation \u00b6 Pearson correlation banyak digunakan dalam data expresi gen. Ukuran similaritas ini menghitung similaritas antara duan bentuk pola expresi gen. Pearson correlation didefinisikan dengan: Pearson(x,y)=\u2211ni=1(xi\u2212\u03bcx)(yi\u2212\u03bcy)\u221a\u2211ni=1(xi\u2212yi)2\u221a\u2211ni=1(xi\u2212yi)2Pearson(x,y)=\u2211i=1n(xi\u2212\u03bcx)(yi\u2212\u03bcy)\u2211i=1n(xi\u2212yi)2\u2211i=1n(xi\u2212yi)2The Pearson correlation kelemahannya adalah sensitif terhadap outlier Mengukur Jarak Tipe Binary \u00b6 Atribut biner merupakan atribut yang hanya memiliki dua status: 0 dan 1. Contoh dari atribut biner adalah hasil tes urine yang akan mendapatkan hasil positiv dan negatif, dimana hasil dari positif representasikan sebagai 1 dan sebaliknya hasil negative representasikan sebagai 0. Dalam menghitung jarak tipe biner tidak diperkenankan menyamakan dengan menghitung jarak tipe numerik ada metode khusus untuk menghitungnya. Jadi, bagaimana kita bisa menghitung ketidaksamaan antara dua atribut biner? \u201dSatu pendekatan melibatkan penghitungan matriks ketidaksamaan dari data biner yang diberikan. Jika semua atribut biner dianggap memiliki bobot yang sama, kita memiliki tabel kontingensi 2\u00d72 di mana q adalah jumlah atribut yang sama dengan 1 untuk kedua objek i dan j, r adalah jumlah atribut yang sama dengan 1 untuk objek i tetapi 0 untuk objek j, s adalah jumlah atribut yang sama dengan 0 untuk objek i tetapi 1 untuk objek j, dan t adalah jumlah atribut yang sama dengan 0 untuk kedua objek i dan j. Jumlah total atribut adalah p, di mana p=q+r+s+t Ingatlah bahwa untuk atribut biner simetris, masing-masing nilai bobot yang sama .Dissimilarity yang didasarkan pada atribut aymmetric binary disebut symmetric binary dissimilarity. Jika objek i dan j dinyatakan sebagai atribut biner simetris, maka dissimilarity antari dan j adalah : d(i,j)=r+sq+r+s+td(i,j)=r+sq+r+s+t Untuk atribut biner asimetris, kedua kondisi tersebut tidak sama pentingnya, seperti hasil positif (1) dan negatif (0) dari tes penyakit. Diberikan dua atribut biner asimetris, pencocokan keduanya 1 (kecocokan positif) kemudian dianggap lebih signifikan daripada kecocokan negatif. Ketidaksamaan berdasarkan atribut-atribut ini disebut asimetris biner dissimilarity, di mana jumlah kecocokan negatif, t, dianggap tidak penting dan dengan demikian diabaikan. Berikut perhitungannya: d(i,j)=r+sq+r+sd(i,j)=r+sq+r+sKita dapat mengukur perbedaan antara dua atribut biner berdasarkan pada disimilarity. Misalnya, biner asimetris kesamaan antara objek i dan j dapat dihitung dengansim(i,j)=qq+r+s=1\u2212d(i,j)sim\u2061(i,j)=qq+r+s=1\u2212d(i,j)Persamaan similarity ini disebut dengan Jaccard coefficient Mengukur Jarak Tipe Kategorical \u00b6 Sebuah data tipe kategorial bisa membawa dua atau lebih pernyataan. Misalnya, map_color sebuah atribut nominal yang mempunya 5 pernyataan yaitu: merah, kuning, hijau, merah jambu, dan biru. Status dapat dilambangkan dengan byletters, simbol, atau satu set bilangan bulat, seperti 1, 2, ..., M. Perhatikan bahwa bilangan bulat tersebut digunakan hanya untuk penanganan data dan tidak mewakili pemesanan khusus apa pun. Perbedaan antara dua objek i dan j bisa di hitung dengan menggunakan rasio ketidak cocokan :d(i,j)=p\u2212mpd(i,j)=p\u2212mpdimana, mm merupakan angka yang cocok (nomer yang cocok untuk ii dan jj ). Dan p merupakan banyak fitur yang di hitung sebagai tipe nominal. Mengukur Jarak Tipe Ordinal \u00b6 Nilai-nilai atribut ordinal memiliki urutan atau peringkat, namun besarnya antara nilai-nilai berturut-turut tidak diketahui. Contohnya tingkatan kecil, sedang, besar untuk atribut ukuran. Atribut ordinal juga dapat diperoleh dari diskritisasi atribut numerik dengan membuat rentang nilai ke dalam sejumlah kategori tertentu. Kategori-kategori ini disusun dalam peringkat. Yaitu, rentang atribut numerik dapat dipetakan ke atribut ordinal ff yang memiliki MfMf state. Misalnya, kisaran suhu atribut skala-skala (dalam Celcius) dapat diatur ke dalam status berikut: \u221230 hingga \u221210, \u221210 hingga 10, 10 hingga 0, masing-masing mewakili kategori suhu dingin, suhu sedang, dan suhu hangat. MM adalah jumlah keadaan yang dapat dilakukan oleh atribut ordinalmemiliki. State ini menentukan peringkat 1,...,Mf1,...,Mf Perlakuan untuk atribut ordinal adalah cukup sama dengan atribut numerik ketika menghitung disimilarity antara objek. Misalkan ff adalah atribut-atribut dari atribut ordinal dari n objek. Menghitung disimilarity terhadap ff fitur sebagai berikut: Nilaiff untuk objek ke-i adalah xifxif, dan f memiliki MfMf status urutan , mewakili peringkat 1,..,Mf1,..,Mf Ganti setiap xifxif dengan peringkatnya, rif\u22081...Mfrif\u22081...Mf Karena setiap atribut ordinal dapat memiliki jumlah state yang berbeda, diperlukan untuk memetakan rentang setiap atribut ke [0,0, 1.0] sehingga setiap atribut memiliki bobot yang sama. Perl melakukan normalisasi data dengan mengganti peringkat rifrif dengan : zif=rif\u22121Mf\u22121zif=rif\u22121Mf\u22121 Dissimilarity kemudian dihitung dengan menggunakan ukuran jarak seperti atribut numerik dengan data yang baru setelah ditransformasi zifzif Mencari Jarak Data Tipe Campuran Menggunakan Python \u00b6 Alat dan Bahan \u00b6 Pada kasus kali ini saya telah menyediakan data tipe campuran yang disimpan dalam bentuk .csv yang dapat di unduh disini . untuk mempermudah dalam penyelesaian kasus ini, perlu di siapkan library dari python untuk mempermudah dalam pengerjaan. Library ini dapat di unduh secara gratis dari internet. Berikut merupakan library yang harus di persiapkan: pandas, digunakan untuk data manajemen dan data analysis. scipy, merupakan library berisi kumpulan algoritma dan fungsi matematika.","title":"Mengukur Jarak Data"},{"location":"Mengukur Jarak Data/#mengukur-jarak-data","text":"","title":"Mengukur Jarak Data"},{"location":"Mengukur Jarak Data/#mengukur-jarak-tipe-numerik","text":"Salah satu tantangan dalam era ini dengan datatabase yang memiliki banyak tipe data. Mengukur jarak adalah komponen utama dalam algoritma clustering berbasis jarak. Alogritma seperti Algoritma Partisioning misal K-Mean, K-medoidm dan fuzzy c-mean dan rough clustering bergantung pada jarak untuk melakukan pengelompokkan. Ada beberapa cara untuk menghitung similaritas atau jarak dari dari data tipe numerik, diantaranya:","title":"Mengukur Jarak Tipe Numerik\u00b6"},{"location":"Mengukur Jarak Data/#minkowski-distance","text":"Kelompok Minkowski diantaranya adalah Euclidean distance dan Manhattan distance, yang menjadi kasus khusus dari Minkowski distance. Minkowski distance dinyatakan dengan: dmin=( sumni=1|xi\u2212yi|m)1m,m\u22651dmin=( sumi=1n|xi\u2212yi|m)1m,m\u22651 Diman mm adalah bilangan riel positif dan xixi dan yiyi adalah dua vektor dalam runang dimensi nn Implementasi ukuran jarak Minkowski pada model clustering data atribut dilakukan normalisasi untuk menghindari dominasi dari atribut yang memiliki skala data besar.","title":"Minkowski Distance\u00b6"},{"location":"Mengukur Jarak Data/#manhattan-distance","text":"Manhattan distance adalah kasus khsusu dari jarak Minkowski distance pada m = 1. Seperti Minkowski Distance, Manhattan distance sensitif terhadap outlier. BIla ukuran ini digunakan dalam algoritma clustering , bentuk cluster adalah hyper-rectangular. Ukuran ini didefinisikan dengan: dman=n\u2211i=1|xi\u2212yi|dman=\u2211i=1n|xi\u2212yi|","title":"Manhattan Distance\u00b6"},{"location":"Mengukur Jarak Data/#euiclidian-distance","text":"Jarak yang paling terkenal yang digunakan untuk data numerik adalah jarak Euclidean. Ini adalah kasus khusus dari jarak Minkowski ketika m = 2. Jarak Euclidean berkinerja baik ketika digunakan untuk kumpulan data cluster kompak atau terisolasi . Meskipun jarak Euclidean sangat umum dalam pengelompokan, ia memiliki kelemahan: jika dua vektor data tidak memiliki nilai atribut yang sama, kemungkin memiliki jarak yang lebih kecil daripada pasangan vektor data lainnya yang mengandung nilai atribut yang sama. Masalah lain dengan jarak Euclidean sebagai fitur skala terbesar akan mendominasi yang lain. Normalisasi fitur kontinu adalah solusi untuk mengatasi kelemahan ini. Euclidian Distance dinyatakan dengan: d(x,y)=\ue001\ue000 \ue000\u23b7n\u2211i=1(xi\u2212yi)2d(x,y)=\u2211i=1n(xi\u2212yi)2","title":"Euiclidian Distance\u00b6"},{"location":"Mengukur Jarak Data/#average-distance","text":"Berkenaan dengan kekurangan dari Jarak Euclidian Distance diatas, rata rata jarak adala versi modikfikasi dari jarak Euclidian untuk memperbaiki hasil. Untuk dua titik x,yx,y dalam ruang dimensi nn, rata-rata jarak didefinisikan dengan: dave=(1nn\u2211i=1(xi\u2212yi)2)12dave=(1n\u2211i=1n(xi\u2212yi)2)12","title":"Average Distance\u00b6"},{"location":"Mengukur Jarak Data/#weighted-euclidean-distance","text":"Jika berdasarkan tingkatan penting dari masing masing atribut ditentukan, maka Weighted Euclidean distance adalah modifikisasi lain dari jarak Euclidean distance yang dapat digunakan. Ukuran ini dirumuskan dengan:dwe=(n\u2211i=1wi(xi\u2212yi)2)12dwe=(\u2211i=1nwi(xi\u2212yi)2)12dimana wi adalah bobot yang diberikan pada atribut ke i.","title":"Weighted euclidean distance\u00b6"},{"location":"Mengukur Jarak Data/#chord-distance","text":"Chord distance adalah salah satu ukuran jarak modifikasi Euclidean distance untuk mengatasi kekurangan dari Euclidean distance. Ini dapat dipecahkan juga dengan menggunakan skala pengukuran yang baik. Jarak ini dapat juga dihitung dari data yang tidak dinormalisasi . Chord distance didefinisikan dengan: dchord=(2\u22122\u2211ni=1xiyi|x|2|y|2)12dchord=(2\u22122\u2211i=1nxiyi|x|2|y|2)12dimana |x|2|x|2 adalah L2\u2212norm|x|2=\u221a\u2211ni=1x2iL2\u2212norm|x|2=\u2211i=1nxi2","title":"Chord distance\u00b6"},{"location":"Mengukur Jarak Data/#mahalanobis-distance","text":"Mahalanobis distance berdasarkan data berbeda dengan Euclidean dan Manhattan distances yang bebas antra data dengan data yang lain. Jarak Mahalanobis yang teratur dapat digunakan untuk mengekstraksi hyperellipsoidal clusters. Jarak Mahalanobis dapat mengurangi distorsi yang disebabkan oleh korelasi linier antara fitur dengan menerapkan transformasi pemutihan ke data atau dengan menggunakan kuadrat Jarak mahalanobis. Mahalanobis distance dinyatakan dengan: dmah=\u221a(x\u2212y)S\u22121(x\u2212y)Tdmah=(x\u2212y)S\u22121(x\u2212y)Tdiman SS adalah matrik covariance data.","title":"Mahalanobis distance\u00b6"},{"location":"Mengukur Jarak Data/#cosine-measure","text":"Ukuran Cosine similarity lebih banyak digunakan dalam similaritas dokumen dan dinyatakan denganCosine(x,y)=\u2211ni=1xiyi|x|2|y|2Cosine(x,y)=\u2211i=1nxiyi|x|2|y|2dimana |y|2|y|2 adalah Euclidean norm dari vektor y=(y1,y2,\u2026,yn)y=(y1,y2,\u2026,yn) di definisikan dengan |y|2=\u221ay21+y22+\u2026+y2n|y|2=y12+y22+\u2026+yn2","title":"Cosine measure\u00b6"},{"location":"Mengukur Jarak Data/#pearson-correlation","text":"Pearson correlation banyak digunakan dalam data expresi gen. Ukuran similaritas ini menghitung similaritas antara duan bentuk pola expresi gen. Pearson correlation didefinisikan dengan: Pearson(x,y)=\u2211ni=1(xi\u2212\u03bcx)(yi\u2212\u03bcy)\u221a\u2211ni=1(xi\u2212yi)2\u221a\u2211ni=1(xi\u2212yi)2Pearson(x,y)=\u2211i=1n(xi\u2212\u03bcx)(yi\u2212\u03bcy)\u2211i=1n(xi\u2212yi)2\u2211i=1n(xi\u2212yi)2The Pearson correlation kelemahannya adalah sensitif terhadap outlier","title":"Pearson Correlation\u00b6"},{"location":"Mengukur Jarak Data/#mengukur-jarak-tipe-binary","text":"Atribut biner merupakan atribut yang hanya memiliki dua status: 0 dan 1. Contoh dari atribut biner adalah hasil tes urine yang akan mendapatkan hasil positiv dan negatif, dimana hasil dari positif representasikan sebagai 1 dan sebaliknya hasil negative representasikan sebagai 0. Dalam menghitung jarak tipe biner tidak diperkenankan menyamakan dengan menghitung jarak tipe numerik ada metode khusus untuk menghitungnya. Jadi, bagaimana kita bisa menghitung ketidaksamaan antara dua atribut biner? \u201dSatu pendekatan melibatkan penghitungan matriks ketidaksamaan dari data biner yang diberikan. Jika semua atribut biner dianggap memiliki bobot yang sama, kita memiliki tabel kontingensi 2\u00d72 di mana q adalah jumlah atribut yang sama dengan 1 untuk kedua objek i dan j, r adalah jumlah atribut yang sama dengan 1 untuk objek i tetapi 0 untuk objek j, s adalah jumlah atribut yang sama dengan 0 untuk objek i tetapi 1 untuk objek j, dan t adalah jumlah atribut yang sama dengan 0 untuk kedua objek i dan j. Jumlah total atribut adalah p, di mana p=q+r+s+t Ingatlah bahwa untuk atribut biner simetris, masing-masing nilai bobot yang sama .Dissimilarity yang didasarkan pada atribut aymmetric binary disebut symmetric binary dissimilarity. Jika objek i dan j dinyatakan sebagai atribut biner simetris, maka dissimilarity antari dan j adalah : d(i,j)=r+sq+r+s+td(i,j)=r+sq+r+s+t Untuk atribut biner asimetris, kedua kondisi tersebut tidak sama pentingnya, seperti hasil positif (1) dan negatif (0) dari tes penyakit. Diberikan dua atribut biner asimetris, pencocokan keduanya 1 (kecocokan positif) kemudian dianggap lebih signifikan daripada kecocokan negatif. Ketidaksamaan berdasarkan atribut-atribut ini disebut asimetris biner dissimilarity, di mana jumlah kecocokan negatif, t, dianggap tidak penting dan dengan demikian diabaikan. Berikut perhitungannya: d(i,j)=r+sq+r+sd(i,j)=r+sq+r+sKita dapat mengukur perbedaan antara dua atribut biner berdasarkan pada disimilarity. Misalnya, biner asimetris kesamaan antara objek i dan j dapat dihitung dengansim(i,j)=qq+r+s=1\u2212d(i,j)sim\u2061(i,j)=qq+r+s=1\u2212d(i,j)Persamaan similarity ini disebut dengan Jaccard coefficient","title":"Mengukur Jarak Tipe Binary\u00b6"},{"location":"Mengukur Jarak Data/#mengukur-jarak-tipe-kategorical","text":"Sebuah data tipe kategorial bisa membawa dua atau lebih pernyataan. Misalnya, map_color sebuah atribut nominal yang mempunya 5 pernyataan yaitu: merah, kuning, hijau, merah jambu, dan biru. Status dapat dilambangkan dengan byletters, simbol, atau satu set bilangan bulat, seperti 1, 2, ..., M. Perhatikan bahwa bilangan bulat tersebut digunakan hanya untuk penanganan data dan tidak mewakili pemesanan khusus apa pun. Perbedaan antara dua objek i dan j bisa di hitung dengan menggunakan rasio ketidak cocokan :d(i,j)=p\u2212mpd(i,j)=p\u2212mpdimana, mm merupakan angka yang cocok (nomer yang cocok untuk ii dan jj ). Dan p merupakan banyak fitur yang di hitung sebagai tipe nominal.","title":"Mengukur Jarak Tipe Kategorical\u00b6"},{"location":"Mengukur Jarak Data/#mengukur-jarak-tipe-ordinal","text":"Nilai-nilai atribut ordinal memiliki urutan atau peringkat, namun besarnya antara nilai-nilai berturut-turut tidak diketahui. Contohnya tingkatan kecil, sedang, besar untuk atribut ukuran. Atribut ordinal juga dapat diperoleh dari diskritisasi atribut numerik dengan membuat rentang nilai ke dalam sejumlah kategori tertentu. Kategori-kategori ini disusun dalam peringkat. Yaitu, rentang atribut numerik dapat dipetakan ke atribut ordinal ff yang memiliki MfMf state. Misalnya, kisaran suhu atribut skala-skala (dalam Celcius) dapat diatur ke dalam status berikut: \u221230 hingga \u221210, \u221210 hingga 10, 10 hingga 0, masing-masing mewakili kategori suhu dingin, suhu sedang, dan suhu hangat. MM adalah jumlah keadaan yang dapat dilakukan oleh atribut ordinalmemiliki. State ini menentukan peringkat 1,...,Mf1,...,Mf Perlakuan untuk atribut ordinal adalah cukup sama dengan atribut numerik ketika menghitung disimilarity antara objek. Misalkan ff adalah atribut-atribut dari atribut ordinal dari n objek. Menghitung disimilarity terhadap ff fitur sebagai berikut: Nilaiff untuk objek ke-i adalah xifxif, dan f memiliki MfMf status urutan , mewakili peringkat 1,..,Mf1,..,Mf Ganti setiap xifxif dengan peringkatnya, rif\u22081...Mfrif\u22081...Mf Karena setiap atribut ordinal dapat memiliki jumlah state yang berbeda, diperlukan untuk memetakan rentang setiap atribut ke [0,0, 1.0] sehingga setiap atribut memiliki bobot yang sama. Perl melakukan normalisasi data dengan mengganti peringkat rifrif dengan : zif=rif\u22121Mf\u22121zif=rif\u22121Mf\u22121 Dissimilarity kemudian dihitung dengan menggunakan ukuran jarak seperti atribut numerik dengan data yang baru setelah ditransformasi zifzif","title":"Mengukur Jarak Tipe Ordinal\u00b6"},{"location":"Mengukur Jarak Data/#mencari-jarak-data-tipe-campuran-menggunakan-python","text":"","title":"Mencari Jarak Data Tipe Campuran Menggunakan Python\u00b6"},{"location":"Mengukur Jarak Data/#alat-dan-bahan","text":"Pada kasus kali ini saya telah menyediakan data tipe campuran yang disimpan dalam bentuk .csv yang dapat di unduh disini . untuk mempermudah dalam penyelesaian kasus ini, perlu di siapkan library dari python untuk mempermudah dalam pengerjaan. Library ini dapat di unduh secara gratis dari internet. Berikut merupakan library yang harus di persiapkan: pandas, digunakan untuk data manajemen dan data analysis. scipy, merupakan library berisi kumpulan algoritma dan fungsi matematika.","title":"Alat dan Bahan\u00b6"},{"location":"Missing Values dengan K-NN/","text":"Mangatasi Missing Values dengan K-NN Missing Values \u00b6 Missing Values (nilai yang hilang) adalah kejadian umum, dimana nilai yang hilang dapat menandakan sejumlah hal berbeda dalam data. Dan mungkin data tidak tersedia atau tidak berlaku. Missing value biasanya disebabkan oleh orang yang memasukkan data dan tidak tahu nilai yang benar, atau tidak mengisinya. Metode penambangan data bervariasi dalam cara mereka memperlakukan nilai yang hilang. Biasanya, mereka mengabaikan nilai yang hilang, atau mengecualikan catatan yang berisi nilai yang hilang, atau mengganti nilai yang hilang dengan nilai tengah, atau menyimpulkan nilai yang hilang dari nilai yang ada. Algoritma K-NN (k-Nearest Neighbors) \u00b6 Algoritma K-Nearest Naighbors adalah suatu algoritma klasifikasi sederhana yang dapat digunakan untuk memprediksi klasifikasi dan regresi. Algoritma ini memiliki tujuan untuk mengklasifikasi objek baru berdasarkan atribut dan sample-sample data training. langkah penyelesaian yang dilakukan oleh algoritma tersebut adalah: Kita harus menentukan jumlah tetangga terdekat yang nantinya akan kita hitung. Misalnya : kita menentukan 2 tetangga terdekat (k=2). Hitung jarak objek yang dipilih dengan seluruh tetangga yang ada. kemudian urutkan berdasarkan jarak yang diperoleh dari yang terkecil hingga ke terbesar. Ambil 2 tetangga yang paling dekat atau nilai jarak yang terkecil, dan ambil rata-ratanya. Mengatasi Missing Value Menggunakan Algoritma K-NN pada Python \u00b6 Alat dan Bahan \u00b6 Pada kasus kali ini, saya menggunakan dataset dari internet, yang bisa diunduh disini . Pada dataset tersebut terdapat 1 fitur bertipe binary dan 5 fitur bertipe numerikal. Pada fitur age dataset telah saya modif dengan mengubah dalam bentuk .csv dan memberikan missing value pada baris ke-257. Untuk mempermudah dalam penyelesaian kasus ini, perlu di siapkan library python untuk mempermudah dalam pengerjaan. Library ini dapat di unduh secara gratis dari internet. Berikut merupakan library yang harus di persiapkan: pandas, digunakan untuk data manajemen dan data analysis. scipy, merupakan library berisi kumpulan algoritma dan fungsi matematika.","title":"Missing Values Dengan K-NN"},{"location":"Missing Values dengan K-NN/#mangatasi-missing-values-dengan-k-nn","text":"","title":"Mangatasi Missing Values dengan K-NN"},{"location":"Missing Values dengan K-NN/#missing-values","text":"Missing Values (nilai yang hilang) adalah kejadian umum, dimana nilai yang hilang dapat menandakan sejumlah hal berbeda dalam data. Dan mungkin data tidak tersedia atau tidak berlaku. Missing value biasanya disebabkan oleh orang yang memasukkan data dan tidak tahu nilai yang benar, atau tidak mengisinya. Metode penambangan data bervariasi dalam cara mereka memperlakukan nilai yang hilang. Biasanya, mereka mengabaikan nilai yang hilang, atau mengecualikan catatan yang berisi nilai yang hilang, atau mengganti nilai yang hilang dengan nilai tengah, atau menyimpulkan nilai yang hilang dari nilai yang ada.","title":"Missing Values\u00b6"},{"location":"Missing Values dengan K-NN/#algoritma-k-nn-k-nearest-neighbors","text":"Algoritma K-Nearest Naighbors adalah suatu algoritma klasifikasi sederhana yang dapat digunakan untuk memprediksi klasifikasi dan regresi. Algoritma ini memiliki tujuan untuk mengklasifikasi objek baru berdasarkan atribut dan sample-sample data training. langkah penyelesaian yang dilakukan oleh algoritma tersebut adalah: Kita harus menentukan jumlah tetangga terdekat yang nantinya akan kita hitung. Misalnya : kita menentukan 2 tetangga terdekat (k=2). Hitung jarak objek yang dipilih dengan seluruh tetangga yang ada. kemudian urutkan berdasarkan jarak yang diperoleh dari yang terkecil hingga ke terbesar. Ambil 2 tetangga yang paling dekat atau nilai jarak yang terkecil, dan ambil rata-ratanya.","title":"Algoritma K-NN (k-Nearest Neighbors)\u00b6"},{"location":"Missing Values dengan K-NN/#mengatasi-missing-value-menggunakan-algoritma-k-nn-pada-python","text":"","title":"Mengatasi Missing Value Menggunakan Algoritma K-NN pada Python\u00b6"},{"location":"Missing Values dengan K-NN/#alat-dan-bahan","text":"Pada kasus kali ini, saya menggunakan dataset dari internet, yang bisa diunduh disini . Pada dataset tersebut terdapat 1 fitur bertipe binary dan 5 fitur bertipe numerikal. Pada fitur age dataset telah saya modif dengan mengubah dalam bentuk .csv dan memberikan missing value pada baris ke-257. Untuk mempermudah dalam penyelesaian kasus ini, perlu di siapkan library python untuk mempermudah dalam pengerjaan. Library ini dapat di unduh secara gratis dari internet. Berikut merupakan library yang harus di persiapkan: pandas, digunakan untuk data manajemen dan data analysis. scipy, merupakan library berisi kumpulan algoritma dan fungsi matematika.","title":"Alat dan Bahan\u00b6"},{"location":"Seleksi Fitur/","text":"Seleksi Fitur Kita dapat menghitung \"seberapa berharga\" fitur X dalam data melalui Feature Gain. Dengan demikian, fitur terlalu banyak bisa dikurangi. from pandas import * from IPython.display import HTML, display from tabulate import tabulate from math import log from sklearn.feature_selection import mutual_info_classif def table(df): display(HTML(tabulate(df, tablefmt='html', headers='keys', showindex=False))) Mari kita ambil beberapa sampel: df = read_csv('play.csv', sep=';') table(df) outlook temperature humidity windy play sunny hot high False no sunny hot high True no overcast hot high False yes rainy mild high False yes rainy cool normal False yes rainy cool normal True no overcast cool normal True yes sunny mild high False no sunny cool normal False yes rainy mild normal False yes sunny mild normal True yes overcast mild high True yes overcast hot normal False yes rainy mild high True no Entropy Target Entropy (keberagaman) kolom target: E(T)=\u2211i=1n\u2212PilogPiE(T)=\u2211i=1n\u2212Pilog\u2061Pi PP = Probability muncul dalam row def findEntropy(column): rawGroups = df.groupby(column) targetGroups = [[key, len(data), len(data)/df[column].size] for key,data in rawGroups] targetGroups = DataFrame(targetGroups, columns=['value', 'count', 'probability']) return sum([-x*log(x,2) for x in targetGroups['probability']]), targetGroups, rawGroups entropyTarget, groupTargets, _ = findEntropy('play') table(groupTargets) print('entropy target =', entropyTarget) value count probability no 5 0.357143 yes 9 0.642857 entropy target = 0.9402859586706309 Gain Gain dalam sebuah fitur (X) untuk data (T): Gain(T,X)=Entropy(T)\u2212\u2211v\u2208TTX,vTE(TX,v)Gain\u2061(T,X)=Entropy\u2061(T)\u2212\u2211v\u2208TTX,vTE(TX,v) def findGain(column): entropyOutlook, groupOutlooks, rawOutlooks = findEntropy(column) table(groupOutlooks) gain = entropyTarget-sum(len(data)/len(df)*sum(-x/len(data)*log(x/len(data),2) for x in data.groupby('play').size()) for key,data in rawOutlooks) print(\"gain of\",column,\"is\",gain) return gain gains = [[x,findGain(x)] for x in ['outlook','temperature','humidity','windy']] value count probability overcast 4 0.285714 rainy 5 0.357143 sunny 5 0.357143 gain of outlook is 0.2467498197744391 value count probability cool 4 0.285714 hot 4 0.285714 mild 6 0.428571 gain of temperature is 0.029222565658954647 value count probability high 7 0.5 normal 7 0.5 gain of humidity is 0.15183550136234136 value count probability False 8 0.571429 True 6 0.428571 gain of windy is 0.04812703040826927 Overall Gain Score: table(DataFrame(gains, columns=[\"Feature\", \"Gain Score\"]).sort_values(\"Gain Score\")[::-1]) Feature Gain Score outlook 0.24675 humidity 0.151836 windy 0.048127 temperature 0.0292226","title":"Seleksi Fitur"},{"location":"Seleksi Fitur/#seleksi-fitur","text":"Kita dapat menghitung \"seberapa berharga\" fitur X dalam data melalui Feature Gain. Dengan demikian, fitur terlalu banyak bisa dikurangi. from pandas import * from IPython.display import HTML, display from tabulate import tabulate from math import log from sklearn.feature_selection import mutual_info_classif def table(df): display(HTML(tabulate(df, tablefmt='html', headers='keys', showindex=False))) Mari kita ambil beberapa sampel: df = read_csv('play.csv', sep=';') table(df) outlook temperature humidity windy play sunny hot high False no sunny hot high True no overcast hot high False yes rainy mild high False yes rainy cool normal False yes rainy cool normal True no overcast cool normal True yes sunny mild high False no sunny cool normal False yes rainy mild normal False yes sunny mild normal True yes overcast mild high True yes overcast hot normal False yes rainy mild high True no","title":"Seleksi Fitur"},{"location":"Seleksi Fitur/#entropy-target","text":"Entropy (keberagaman) kolom target: E(T)=\u2211i=1n\u2212PilogPiE(T)=\u2211i=1n\u2212Pilog\u2061Pi PP = Probability muncul dalam row def findEntropy(column): rawGroups = df.groupby(column) targetGroups = [[key, len(data), len(data)/df[column].size] for key,data in rawGroups] targetGroups = DataFrame(targetGroups, columns=['value', 'count', 'probability']) return sum([-x*log(x,2) for x in targetGroups['probability']]), targetGroups, rawGroups entropyTarget, groupTargets, _ = findEntropy('play') table(groupTargets) print('entropy target =', entropyTarget) value count probability no 5 0.357143 yes 9 0.642857 entropy target = 0.9402859586706309","title":"Entropy Target"},{"location":"Seleksi Fitur/#gain","text":"Gain dalam sebuah fitur (X) untuk data (T): Gain(T,X)=Entropy(T)\u2212\u2211v\u2208TTX,vTE(TX,v)Gain\u2061(T,X)=Entropy\u2061(T)\u2212\u2211v\u2208TTX,vTE(TX,v) def findGain(column): entropyOutlook, groupOutlooks, rawOutlooks = findEntropy(column) table(groupOutlooks) gain = entropyTarget-sum(len(data)/len(df)*sum(-x/len(data)*log(x/len(data),2) for x in data.groupby('play').size()) for key,data in rawOutlooks) print(\"gain of\",column,\"is\",gain) return gain gains = [[x,findGain(x)] for x in ['outlook','temperature','humidity','windy']] value count probability overcast 4 0.285714 rainy 5 0.357143 sunny 5 0.357143 gain of outlook is 0.2467498197744391 value count probability cool 4 0.285714 hot 4 0.285714 mild 6 0.428571 gain of temperature is 0.029222565658954647 value count probability high 7 0.5 normal 7 0.5 gain of humidity is 0.15183550136234136 value count probability False 8 0.571429 True 6 0.428571 gain of windy is 0.04812703040826927","title":"Gain"},{"location":"Seleksi Fitur/#overall-gain-score","text":"table(DataFrame(gains, columns=[\"Feature\", \"Gain Score\"]).sort_values(\"Gain Score\")[::-1]) Feature Gain Score outlook 0.24675 humidity 0.151836 windy 0.048127 temperature 0.0292226","title":"Overall Gain Score:"},{"location":"Statistik deskriptif/","text":"Statistik deskriptif Pengertian Statistika deskriptif adalah metode pengumpulan dan penyajian data menjadi suatu gugus data sehingga menghasilkan informasi yang berguna. \u200b Statistika deskriptif hanya memberikan data yang dia punyai dan sama sekali tidak menarik kesimpulan apapun tentang gugus induknya yang lebih besar. Contoh statistika deskriptif adalah data yang kumpulan data yang di peroleh akan tersaji dengan ringkas dan rapi serta dapat memberikan informasi inti dari kumpulan data yang ada seperti data table, diagram, grafik, dan besaran-besaran lainya di majalah atau koran yang memuat data seperti: ukuran pemusatan data, ukuran penyebaran data, serta kecenderungan suatu gugus data. Tipe Statistik Deskriptif Mean(rata-rata) \u200b Mean adalah nilai rata-rata dari beberapa buah data. Nilai mean dapat ditentukan dengan membagi jumlah data dengan banyaknya data. \u200b Mean (rata-rata) merupakan suatu ukuran pemusatan data. Mean suatu data juga merupakan statistik karena mampu menggambarkan bahwa data tersebut berada pada kisaran mean data tersebut. Mean tidak dapat digunakan sebagai ukuran pemusatan untuk jenis data nominal dan ordinal. \u200b Berdasarkan definisi dari mean adalah jumlah seluruh data dibagi dengan banyaknya data. Dengan kata lain jika kita memiliki N data sebagai berikut maka mean data tersebut dapat kita tuliskan sebagai berikut : Dimana: x = data ke n x bar = x rata-rata = nilai rata-rata sampel n = banyaknya data Median \u200b Median menentukan letak tengah data setelah data disusun menurut urutan nilainya. Bisa juga *nilai tengah dari data-data yang terurut. Simbol untuk median adalah Me. Dengan median Me, maka 50% dari banyak data nilainya paling tinggi sama dengan Me, dan 50% dari banyak data nilainya paling rendah sama dengan Me. Dalam mencari median, dibedakan untuk banyak data ganjil dan banyak data genap. Untuk banyak data ganjil, setelah data disusun menurut nilainya, maka median Me adalah data yang terletak tepat di tengah. Median bisa dihitung menggunakan rumus sebagai berikut: Modus \u200b Modus adalah nilai yang sering muncul. Jika kita tertarik pada data frekuensi, jumlah dari suatu nilai dari kumpulan data, maka kita menggunakan modus. Modus sangat baik bila digunakan untuk data yang memiliki sekala kategorik yaitu nominal atau ordinal. Modus bisa dihitung menggunakan rumus sebagai berikut: Dimana : \u200b Mo = Modus L = Tepi bawah kelas yang memiliki frekuensi tertinggi (kelas modus) i = Interval kelas b1 = Frekuensi kelas modus dikurangi frekuensi kelas interval terdekat sebelumnya b2 = frekuensi kelas modus dikurangi frekuensi kelas interval terdekat sesudahnya Standart Deviasi \u200b Standar Deviasi dan Varians Salah satu teknik statistik yg digunakan untuk menjelaskan homogenitas kelompok. Varians merupakan jumlah kuadrat semua deviasi nilai-nilai individual terhadap rata-rata kelompok. Sedangkan akar dari varians disebut dengan standar deviasi atau simpangan baku. \u200b Standar Deviasi dan Varians Simpangan baku merupakan variasi sebaran data. Semakin kecil nilai sebarannya berarti variasi nilai data makin sama Jika sebarannya bernilai 0, maka nilai semua datanya adalah sama. Semakin besar nilai sebarannya berarti data semakin bervariasi. Standar Deviasi bisa didapat menggunakan rumus sebagai berikut: Dimana : x = data ke n x bar = x rata-rata = nilai rata-rata sampel n = banyaknya data Varians \u200b Varians merupakan rata-rata dari selisih kuadrat tersebut merupakan suatu ukuran penyimpangan dari observasi. Simbol varians pada ukuran populasi sigma kuarat dan pada ukuran sample S2. Akar dari varians dinamakan standar deviasi atau simpangan baku. Varians bisa didapat menggunakan rumus sebagai berikut: Skewness \u200b Skewness ( kemencengan ) adalah derajat ketidaksimetrisan suatu distribusi. Jika kurva frekuensi suatu distribusi memiliki ekor yang lebih memanjang ke kanan (dilihat dari meannya) maka dikatakan menceng kanan (positif) dan jika sebaliknya maka menceng kiri (negatif). Secara perhitungan, skewness adalah momen ketiga terhadap mean. Distribusi normal (dan distribusi simetris lainnya, misalnya distribusi t atau Cauchy) memiliki skewness 0 (nol). Skewness bisa dihitung menggunakan rumus sebagai berikut: Quartile \u200b Quartile adalah nilai-nilai yang membagi segugus pengamatan menjadi empat bagian sama besar. Nilai-nilai itu, yang dilambangkan dengan Q1, Q2, dan Q3, mempunyai sifat bahwa 25% data jatuh dibawah Q1, 50% data jatuh dibawah Q2, dan 75% data jatuh dibawah Q3. Quartile bisa dihitung menggunakan rumus sebagai berikut: Penerapan statistik deskriptif menggunakan python Alat dan Bahan \u200b Pada penerapan ini saya menggunakan 500 data random yang disimpan dalam bentuk .xlsx dan untuk mempermudah dalam penerapan tersebut, perlu disiapkan library python yang dapat didownload secara gratis. dalam kasus ini library python yang digunakan adalah sebagai berikut: pandas, digunakan untuk data manajemen dan data analysis. scipy, berisi kumpulan algoritme dan fungsi matematika. Pertama pada langkah ini kita memasukkan library yang telah disiapkan sebelumya import pandas as pd from scipy import stats Kedua Dan Selanjutnya memuat data excel yang telah di siapkkan df = pd.read_csv('sample_data.csv', sep=';') Ketiga \u200b kemudian membuat data penyimpanan ( dictionary ) yang menampung nilai yang akan ditampilkan. selanjutnya mengambil data dari beberapa kolom pada csv dengan cara diiterasi serta menghitungnya dengan berbagai metode yang telah disiapkan oleh pandas itu sendiri. kemudian hasil tersebut di disimpan pada penyimpanan tadi data = {\"Stats\" : ['Min','Max','Mean','Standard Deviasi','Variasi','Skewnes', 'Quartile 1','Quartile 2', 'Quartile 3', 'Median','Modus']} for i in df.columns: data[i] = [df[i].min(), df[i].max(), df[i].mean(), round(df[i].std(), 2),round(df[i].var(), 2), round(df[i].skew(), 2), df[i].quantile(0.25), df[i].quantile(0.5), df[i].quantile(0.75), df[i].median(), stats.mode(df[i]).mode[0]] Keempat Terakhir adalah menvisualisasikan hasil tersebut dalam bentuk dataframe tes = pd.DataFrame(data, columns = ['Stats'] + [x for x in df.columns]) tes Mencari Outliner \u200b Outlier merupakan suatu nilai dari pada sekumpulan data yang lain atau berbeda dibandingkan biasanya serta tidak menggambarkan karakteristik data tersebut. Sebuah outlier mungkin karena variabilitas dalam pengukuran atau mungkin menunjukkan kesalahan eksperimental. Standarisasi Data deteksi data dengan standarisasi pada prinsipnya mengubah niali data menjadi bentuk Z, dengan menggunakan formula dari Z score, yaitu: pada data xlsx tersebut saya lakukan modifikasi dengan memberikan data sampah yang pada berbagai kolom. dan dalam pencarian outlier ini saya menggunakan formula dari z score yang diterapkan pada python, yaitu sebagai berikut: def dekteksi_outlier(df_in): outliers = [] threshold = 3 for col_name in df_in: mean = df_in[col_name].mean() std = df_in[col_name].std() i=1 for y in df_in[col_name]: z_score = (y - mean) / std if abs(z_score) > threshold: outliers.append([col_name,y,i]) i+=1 return outliers for i in dekteksi_outlier(df): print('Data sampah',i[1],'dikolom',i[0],'pada baris',i[2]) Dari Fungsi diatas menghasilkan outliner dari data xlsx tersebut","title":"Statistic Descriptif"},{"location":"Statistik deskriptif/#statistik-deskriptif","text":"","title":"Statistik deskriptif"},{"location":"Statistik deskriptif/#pengertian","text":"Statistika deskriptif adalah metode pengumpulan dan penyajian data menjadi suatu gugus data sehingga menghasilkan informasi yang berguna. \u200b Statistika deskriptif hanya memberikan data yang dia punyai dan sama sekali tidak menarik kesimpulan apapun tentang gugus induknya yang lebih besar. Contoh statistika deskriptif adalah data yang kumpulan data yang di peroleh akan tersaji dengan ringkas dan rapi serta dapat memberikan informasi inti dari kumpulan data yang ada seperti data table, diagram, grafik, dan besaran-besaran lainya di majalah atau koran yang memuat data seperti: ukuran pemusatan data, ukuran penyebaran data, serta kecenderungan suatu gugus data.","title":"Pengertian"},{"location":"Statistik deskriptif/#tipe-statistik-deskriptif","text":"","title":"Tipe Statistik Deskriptif"},{"location":"Statistik deskriptif/#meanrata-rata","text":"\u200b Mean adalah nilai rata-rata dari beberapa buah data. Nilai mean dapat ditentukan dengan membagi jumlah data dengan banyaknya data. \u200b Mean (rata-rata) merupakan suatu ukuran pemusatan data. Mean suatu data juga merupakan statistik karena mampu menggambarkan bahwa data tersebut berada pada kisaran mean data tersebut. Mean tidak dapat digunakan sebagai ukuran pemusatan untuk jenis data nominal dan ordinal. \u200b Berdasarkan definisi dari mean adalah jumlah seluruh data dibagi dengan banyaknya data. Dengan kata lain jika kita memiliki N data sebagai berikut maka mean data tersebut dapat kita tuliskan sebagai berikut : Dimana: x = data ke n x bar = x rata-rata = nilai rata-rata sampel n = banyaknya data","title":"Mean(rata-rata)"},{"location":"Statistik deskriptif/#median","text":"\u200b Median menentukan letak tengah data setelah data disusun menurut urutan nilainya. Bisa juga *nilai tengah dari data-data yang terurut. Simbol untuk median adalah Me. Dengan median Me, maka 50% dari banyak data nilainya paling tinggi sama dengan Me, dan 50% dari banyak data nilainya paling rendah sama dengan Me. Dalam mencari median, dibedakan untuk banyak data ganjil dan banyak data genap. Untuk banyak data ganjil, setelah data disusun menurut nilainya, maka median Me adalah data yang terletak tepat di tengah. Median bisa dihitung menggunakan rumus sebagai berikut:","title":"Median"},{"location":"Statistik deskriptif/#modus","text":"\u200b Modus adalah nilai yang sering muncul. Jika kita tertarik pada data frekuensi, jumlah dari suatu nilai dari kumpulan data, maka kita menggunakan modus. Modus sangat baik bila digunakan untuk data yang memiliki sekala kategorik yaitu nominal atau ordinal. Modus bisa dihitung menggunakan rumus sebagai berikut: Dimana : \u200b Mo = Modus L = Tepi bawah kelas yang memiliki frekuensi tertinggi (kelas modus) i = Interval kelas b1 = Frekuensi kelas modus dikurangi frekuensi kelas interval terdekat sebelumnya b2 = frekuensi kelas modus dikurangi frekuensi kelas interval terdekat sesudahnya","title":"Modus"},{"location":"Statistik deskriptif/#standart-deviasi","text":"\u200b Standar Deviasi dan Varians Salah satu teknik statistik yg digunakan untuk menjelaskan homogenitas kelompok. Varians merupakan jumlah kuadrat semua deviasi nilai-nilai individual terhadap rata-rata kelompok. Sedangkan akar dari varians disebut dengan standar deviasi atau simpangan baku. \u200b Standar Deviasi dan Varians Simpangan baku merupakan variasi sebaran data. Semakin kecil nilai sebarannya berarti variasi nilai data makin sama Jika sebarannya bernilai 0, maka nilai semua datanya adalah sama. Semakin besar nilai sebarannya berarti data semakin bervariasi. Standar Deviasi bisa didapat menggunakan rumus sebagai berikut: Dimana : x = data ke n x bar = x rata-rata = nilai rata-rata sampel n = banyaknya data","title":"Standart Deviasi"},{"location":"Statistik deskriptif/#varians","text":"\u200b Varians merupakan rata-rata dari selisih kuadrat tersebut merupakan suatu ukuran penyimpangan dari observasi. Simbol varians pada ukuran populasi sigma kuarat dan pada ukuran sample S2. Akar dari varians dinamakan standar deviasi atau simpangan baku. Varians bisa didapat menggunakan rumus sebagai berikut:","title":"Varians"},{"location":"Statistik deskriptif/#skewness","text":"\u200b Skewness ( kemencengan ) adalah derajat ketidaksimetrisan suatu distribusi. Jika kurva frekuensi suatu distribusi memiliki ekor yang lebih memanjang ke kanan (dilihat dari meannya) maka dikatakan menceng kanan (positif) dan jika sebaliknya maka menceng kiri (negatif). Secara perhitungan, skewness adalah momen ketiga terhadap mean. Distribusi normal (dan distribusi simetris lainnya, misalnya distribusi t atau Cauchy) memiliki skewness 0 (nol). Skewness bisa dihitung menggunakan rumus sebagai berikut:","title":"Skewness"},{"location":"Statistik deskriptif/#quartile","text":"\u200b Quartile adalah nilai-nilai yang membagi segugus pengamatan menjadi empat bagian sama besar. Nilai-nilai itu, yang dilambangkan dengan Q1, Q2, dan Q3, mempunyai sifat bahwa 25% data jatuh dibawah Q1, 50% data jatuh dibawah Q2, dan 75% data jatuh dibawah Q3. Quartile bisa dihitung menggunakan rumus sebagai berikut:","title":"Quartile"},{"location":"Statistik deskriptif/#penerapan-statistik-deskriptif-menggunakan-python","text":"","title":"Penerapan statistik deskriptif menggunakan python"},{"location":"Statistik deskriptif/#alat-dan-bahan","text":"\u200b Pada penerapan ini saya menggunakan 500 data random yang disimpan dalam bentuk .xlsx dan untuk mempermudah dalam penerapan tersebut, perlu disiapkan library python yang dapat didownload secara gratis. dalam kasus ini library python yang digunakan adalah sebagai berikut: pandas, digunakan untuk data manajemen dan data analysis. scipy, berisi kumpulan algoritme dan fungsi matematika.","title":"Alat dan Bahan"},{"location":"Statistik deskriptif/#pertama","text":"pada langkah ini kita memasukkan library yang telah disiapkan sebelumya import pandas as pd from scipy import stats","title":"Pertama"},{"location":"Statistik deskriptif/#kedua","text":"Dan Selanjutnya memuat data excel yang telah di siapkkan df = pd.read_csv('sample_data.csv', sep=';')","title":"Kedua"},{"location":"Statistik deskriptif/#ketiga","text":"\u200b kemudian membuat data penyimpanan ( dictionary ) yang menampung nilai yang akan ditampilkan. selanjutnya mengambil data dari beberapa kolom pada csv dengan cara diiterasi serta menghitungnya dengan berbagai metode yang telah disiapkan oleh pandas itu sendiri. kemudian hasil tersebut di disimpan pada penyimpanan tadi data = {\"Stats\" : ['Min','Max','Mean','Standard Deviasi','Variasi','Skewnes', 'Quartile 1','Quartile 2', 'Quartile 3', 'Median','Modus']} for i in df.columns: data[i] = [df[i].min(), df[i].max(), df[i].mean(), round(df[i].std(), 2),round(df[i].var(), 2), round(df[i].skew(), 2), df[i].quantile(0.25), df[i].quantile(0.5), df[i].quantile(0.75), df[i].median(), stats.mode(df[i]).mode[0]]","title":"Ketiga"},{"location":"Statistik deskriptif/#keempat","text":"Terakhir adalah menvisualisasikan hasil tersebut dalam bentuk dataframe tes = pd.DataFrame(data, columns = ['Stats'] + [x for x in df.columns]) tes","title":"Keempat"},{"location":"Statistik deskriptif/#mencari-outliner","text":"\u200b Outlier merupakan suatu nilai dari pada sekumpulan data yang lain atau berbeda dibandingkan biasanya serta tidak menggambarkan karakteristik data tersebut. Sebuah outlier mungkin karena variabilitas dalam pengukuran atau mungkin menunjukkan kesalahan eksperimental. Standarisasi Data deteksi data dengan standarisasi pada prinsipnya mengubah niali data menjadi bentuk Z, dengan menggunakan formula dari Z score, yaitu: pada data xlsx tersebut saya lakukan modifikasi dengan memberikan data sampah yang pada berbagai kolom. dan dalam pencarian outlier ini saya menggunakan formula dari z score yang diterapkan pada python, yaitu sebagai berikut: def dekteksi_outlier(df_in): outliers = [] threshold = 3 for col_name in df_in: mean = df_in[col_name].mean() std = df_in[col_name].std() i=1 for y in df_in[col_name]: z_score = (y - mean) / std if abs(z_score) > threshold: outliers.append([col_name,y,i]) i+=1 return outliers for i in dekteksi_outlier(df): print('Data sampah',i[1],'dikolom',i[0],'pada baris',i[2]) Dari Fungsi diatas menghasilkan outliner dari data xlsx tersebut","title":"Mencari Outliner"}]}